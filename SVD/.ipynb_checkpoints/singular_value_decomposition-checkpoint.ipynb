{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**\"SVD is not nearly as famous as it should be.\" - Gilbert Strang**\n",
    "\n",
    "When we think about dimentionality reduction and in particular matrix decomposition \"Singular Value decomposition\" is what comes to mind. In this post we will dive deeper into its computations and parallely apply in it to a text dataset. \n",
    "\n",
    "\n",
    "The SVD factorizes a matrix into one matrix with orthogonal columns and one with orthogonal rows (along with a diagonal matrix, which contains the relative importance of each factor).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD can be simply written as;\n",
    "\n",
    "\\begin{equation*}\\label{eq:}\n",
    "A_{[m \\times n]} = U_{[m \\times r]} S_{[r \\times r]} (V_{[n \\times r]})\n",
    "\\end{equation*}\n",
    "\n",
    "Lets look at each element of the above matrix decomposition. \n",
    "\n",
    "- *$A$ : Input Data matrix*. $[m \\times n]$ : eg. $m$ movies and $n$ users. (See example below)\n",
    "\n",
    "We can think about it as an the input as a Matrix $A$ is of size $m \\times n$, which means it has $m$ rows and $n$ columns. \n",
    "Matrix $A$ can be thought of as a *Document Matrix* with $m$ documents and $n$ terms in it. \n",
    "That means every row reprements a document and every column reprements a word in it. \n",
    "\n",
    "Every document is represented as on long vector with $0$ and $1$ meaning the given word appears or does not appear in the document. We will see this in the below newsgroup dataset. \n",
    "\n",
    "*Alternatively* consider the matrix as a *movie user matrix* such that every row as a different user and every column as a different movie. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decomposition\n",
    "The idea is to take the matrix $A$ and represent it as product of 3 matrices, $u$, $s$ and $v$. This idea of taking the original matrix and representating it as a product of three different matrices is singular value decomposition. Specifically the matrix has few properties. \n",
    "\n",
    "- *$U$ : Left Singular vectors*.  $[m \\times r]$ ($m$ movies and $r$ concepts) . We can think of $r$ concepts as a very small number. We will come back to this later. \n",
    "\n",
    "- *$S$: Singular values*. $[r \\times r]$ diagonal matrix. It basically represents the strength of the matrix $A$. Here we have only non-zero elements across the diagonal which we call as singular values. The singlular values are sorted in the decreasing order. \n",
    "\n",
    "- *$V$: Right Singular values*. $[n \\times r]$ matrix where $n$ is the number of columns from the original matrix and $r$ we can think of as a small number basically the rank of the matrix $A$. \n",
    "\n",
    "We can pictorically represent this as below <img src=\"img/matrix_decom.png\" alt=\"eval\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Properties\n",
    "\n",
    "The \"SVD\" theorem says that \"It is always possible to decompose a real matrix $A$ into $A = USV^T$ where, \n",
    "\n",
    "- $U$, $S$, $V$ are unique.\n",
    "- $U$ and $V$ are *column orthonomal* i.e $U^T U = I$, $V^T V = I$ $I$ : Identity matrix. \n",
    "- $S$ Entries in the singular values are positive and sorted in descreasing order. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "Consider the Matrix user to movie ![matrix](img/movie_user.png \"User to movies\"). \n",
    "\n",
    "Lets think of this example as a movie review website.  Here each row represents user and the column represents different movie. \n",
    "1 Being the lowest and 5 being the higest rating. \n",
    "\n",
    "So a user say no.3 likes more of Scify moves as compared to War movies, hence the column 4 and 5 are 0. \n",
    "\n",
    "Our goal here is to deompose this matrix into three components. Visually we can see that the users can be broken down into two groups as seen below. \n",
    "<img src=\"img/movie_user_grp.png\" alt=\"eval\" width=\"500\"/>\n",
    "\n",
    "Let's demonstrate this with the standard scipy object `linalg` where svd is defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy import linalg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 0, 0],\n",
       "        [3, 3, 3, 0, 0],\n",
       "        [4, 4, 4, 0, 0],\n",
       "        [5, 5, 5, 0, 0],\n",
       "        [0, 2, 0, 4, 4],\n",
       "        [0, 0, 0, 5, 5],\n",
       "        [0, 1, 0, 2, 2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= np.matrix('1 1 1 0 0; 3 3 3 0 0; 4 4 4 0 0 ; 5 5 5 0 0; 0 2 0 4 4; 0 0 0 5 5; 0 1 0 2 2')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of original matrix: (7, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of original matrix:\" , a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, v = linalg.svd(a, full_matrices=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strength of the scifi concept. Here we see that the strength of the scify concept is more than the \"War movie\" concept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.481,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  9.509,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  1.346,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.diag(s), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7, 5), (5,), (5, 5))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape, s.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User to concept** matrix. \n",
    "\n",
    "Matix $U$ we see that the first four users belong to more of SciFy concepy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.14, -0.02, -0.01,  0.56, -0.38],\n",
       "       [-0.41, -0.07, -0.03,  0.21,  0.76],\n",
       "       [-0.55, -0.09, -0.04, -0.72, -0.18],\n",
       "       [-0.69, -0.12, -0.05,  0.34, -0.23],\n",
       "       [-0.15,  0.59,  0.65,  0.  ,  0.2 ],\n",
       "       [-0.07,  0.73, -0.68,  0.  ,  0.  ],\n",
       "       [-0.08,  0.3 ,  0.33,  0.  , -0.4 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "np.round(U, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix $V$ we can relate to as a **\"movie-to-concept\" matrix** as the first three refers to more of the first concept (scifi) whereas the last 2 (0.69) relate to \"war movie\" concept.  \n",
    "\n",
    "In both cases we see we see the third concept which is more or less modelled as \"noise\" in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.56, -0.59, -0.56, -0.09, -0.09],\n",
       "       [-0.13,  0.03, -0.13,  0.69,  0.69],\n",
       "       [-0.41,  0.81, -0.41, -0.09, -0.09],\n",
       "       [-0.71,  0.  ,  0.71,  0.  ,  0.  ],\n",
       "       [ 0.  , -0.  ,  0.  , -0.71,  0.71]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "np.round(v, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  SVD vs Fast Ramdomised SVD\n",
    "\n",
    "The good thing about SVD is that we have a method that allows us to exactly factor a matrix into orthogonal columns and orthogonal rows. Lets demonstrate this in our news group dataset inbuilt in sklearn\n",
    "\n",
    "Newsgroups are discussion groups, which was popular in the 80s and 90s. This dataset includes 18,000 newsgroups posts with 20 different topics. We would like to find topics which are Orthogonal. \n",
    "\n",
    "Now our idea is that, We would clearly expect that the words that appear most frequently in one topic would appear less frequently in the other - otherwise that word wouldn't make a good choice to separate out the two topics. Therefore, we expect the topics to be orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26576)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import decomposition\n",
    "import fbpca\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english') \n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data).todense() # (documents, vocab)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 39.6 s\n"
     ]
    }
   ],
   "source": [
    "%time u, s, v = np.linalg.svd(vectors, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "%time u, s, v = decomposition.randomized_svd(vectors, n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "# using facebook's pca \n",
    "\n",
    "%time u, s, v = fbpca.pca(vectors, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.6 , -0.81, -0.07, ..., -0.65, -1.24,  0.46],\n",
       "       [-0.99, -0.38,  1.11, ...,  0.61, -0.21,  0.24],\n",
       "       [ 0.92,  1.06, -0.56, ...,  0.36, -0.87, -0.14],\n",
       "       ...,\n",
       "       [-1.41, -1.21, -0.09, ..., -0.04,  0.52,  0.39],\n",
       "       [ 0.31,  0.34,  1.51, ...,  0.11, -0.47, -0.91],\n",
       "       [ 0.05, -0.02,  0.22, ...,  0.52, -3.75,  0.09]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(size=(vectors.shape[1], 40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the Randomes approach to SVD is much faster. Lets discuss the method and its implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Similarity\n",
    "\n",
    "Before we move into Randomised SVD's lets find out the similar topics using above svd vectors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26576,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english') #, tokenizer=LemmaTokenizer())\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data).todense()\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cosmonauts', 'cosmos', 'cosponsored', 'cost', 'costa', 'costar',\n",
       "       'costing', 'costly', 'costruction', 'costs'], dtype='<U80')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[7000:7010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words=8\n",
    "\n",
    "def show_topics(a):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jpeg image edu file graphics images gif data',\n",
       " 'edu graphics data space pub mail 128 3d',\n",
       " 'space jesus launch god people satellite matthew atheists',\n",
       " 'jesus god matthew people atheists atheism does graphics',\n",
       " 'image data processing analysis software available tools display',\n",
       " 'jesus matthew prophecy messiah psalm isaiah david said',\n",
       " 'launch commercial satellite market image services satellites launches',\n",
       " 'data available nasa ftp grass anonymous contact gov',\n",
       " 'argument fallacy conclusion example true ad argumentum premises',\n",
       " 'space larson image theory universe physical nasa material']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(v[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With SVD we can do some basic text grouping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized Matrix Approximation\n",
    "\n",
    "## Need for a Randomized Approach \n",
    "\n",
    "Matrix decomposition remains a fundamental approach in many machine learning tasks especially with the advent of NLP. With the development of new applications in the field of Deep learning, the classical algorithms are inadequate to tackle huge tasks. Why? \n",
    "\n",
    "- Matrices are  enormously big. Classical algorithms are not always well adapted to solve the type of large-scale problems that now arise in Deep learning. \n",
    "\n",
    "- More often than not Data are missing. Traditional algorithms which produce accurate Matrix decomposition but ends up in using extra computational resources. \n",
    "\n",
    "- Passes over data needs to be faster, since data transfter plays an important role. For this GPU's can be effectively utilized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method \n",
    "\n",
    "Randomised approach to matrix decomposition was discussed in the [paper, Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions](https://arxiv.org/abs/0909.4061) by Nathan Halko, Per-Gunnar Martinsson and Joel A. Tropp and later summarized by Facebook [in this blog post](https://research.fb.com/fast-randomized-svd/). The method was proposed for general purpose algorithms used for various matrix approximation tasks. \n",
    "\n",
    "**Idea:**  Use a smaller matrix (with smaller $n$)!\n",
    "\n",
    "Instead of calculating the SVD on our full matrix $A$ which is $[m \\times n]$, we use $B = AQ$, which is a $[m \\times r]$ matrix where $r << n$. \n",
    "                                                                                                                          \n",
    "**Note:**  This is just a method with a smaller matrix!!                                                                   \n",
    "\n",
    "\n",
    "1\\. Compute an approximation to the range of $A$. That is, we want $Q$ with $r$ orthonormal columns such that $A \\approx QQ^TA$\n",
    "\n",
    "2\\. Construct $B = Q^T A$, which is small ($r\\times n$)\n",
    "\n",
    "3\\. Compute the SVD of $B$ by standard methods i.e  $B = S\\,\\Sigma V^T$. This is fast since $B$ is smaller than $A$. \n",
    "\n",
    "4\\.\n",
    "\n",
    "\\begin{align*}\n",
    "A &\\approx Q Q^T A \\\\\n",
    " &= Q (S\\,\\Sigma V^T) \\space \\text{as} \\space B = Q^T A \\\\\n",
    " &= U \\Sigma V^T \\space \\text{if we construct} \\space U = QS \\\\\n",
    "\\end{align*}\n",
    "\n",
    "We now have a low rank approximation $A \\approx U \\Sigma V^T$.\n",
    "\n",
    "**Trick in finding Q**\n",
    "\n",
    "To estimate the range of $A$, we can just take a bunch of random vectors $\\omega_i$, evaluate the subspace formed by $A\\omega_i$.  We can form a matrix $\\Omega$ with the $\\omega_i$ as it's columns.  \n",
    "\n",
    "Now, we take the QR decomposition of $A\\Omega = QR$, then the columns of $Q$ form an orthonormal basis for $A\\Omega$, which is the range of $A$.\n",
    "\n",
    "Since the matrix $A\\Omega$ of the product has far more rows than columns and therefore, approximately, orthonormal columns. This is simple probability - with lots of rows, and few columns, it's unlikely that the columns are linearly dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_range_finder(A, size, n_iter=5):\n",
    "    Q = np.random.normal(size=(A.shape[1], size))\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        Q, _ = linalg.lu(A @ Q, permute_l=True)\n",
    "        Q, _ = linalg.lu(A.T @ Q, permute_l=True)\n",
    "        \n",
    "    Q, _ = linalg.qr(A @ Q, mode='economic')\n",
    "    return Q\n",
    "\n",
    "def randomized_svd(M, n_components, n_oversamples=10, n_iter=4):\n",
    "    \n",
    "    n_random = n_components + n_oversamples\n",
    "    \n",
    "    Q = randomized_range_finder(M, n_random, n_iter)\n",
    "    \n",
    "    # project M to the (k + p) dimensional space using the basis vectors\n",
    "    B = Q.T @ M\n",
    "    \n",
    "    # compute the SVD on the thin matrix: (k + p) wide\n",
    "    Uhat, s, V = linalg.svd(B, full_matrices=False)\n",
    "    del B\n",
    "    U = Q @ Uhat\n",
    "    \n",
    "    return U[:, :n_components], s[:n_components], V[:n_components, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%time u, s, v = randomized_svd(vectors, n_components=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Readings\n",
    "\n",
    "- [Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions](https://arxiv.org/abs/0909.4061) is an excellent read. \n",
    "\n",
    "- [FastAi Numerical Linear Algebra](https://www.fast.ai/2017/07/17/num-lin-alg/) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
